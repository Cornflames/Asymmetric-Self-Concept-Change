\section{Reproduktion von Studie 4}
\label{sec:repro}

\Textcite{brotzeller_exploring_2025} kommen aufgrund der Ergebnisse von Studie 4 zu dem Schluss, dass ihre Theorie die Realität nicht adäquat abbildet. Die Gültigkeit dieser Schlussfolgerung hängt jedoch vor jeder Diskussion darüber, ob der gewählte Weg zur Falsifizierung der Theorie der richtige ist, davon ab, ob die Ergebnisse der Studie verlässlich sind.

Die Verlässlichkeit von Ergebnissen lässt sich überprüfen, indem man bestimmte Phasen des Forschungsprozesses, der sie hervorgebracht hat, noch einmal wiederholt. Können solche Redoing"=Activities  übereinstimmende Ergebnisse produzieren, spricht das für die Verlässlichkeit der in einem Artikel berichteten Ergebnisse \autocite{kohrt_conceptual_2024}.

Die Unsicherheit darüber, ob die berichteten Ergebnisse verlässlich sind, speist sich aus zwei Quellen. Zum einen werden die Ergebnisse anhand einer Zufallsstichprobe berechnet, die womöglich nicht repräsentativ für die Gesamtpopulation ist. Zum anderen können Forschern während einer Berechnung Fehler unterlaufen, die zu falschen Ergebnissen führen. Bei sogenannten Reproduktionen wird die in einem Artikel beschriebene Berechnung an der von den Autoren erhobenen Stichprobe wiederholt. Weil man anders als bei einer Replikation der Studie keine neue Stichprobe erhebt, sind Abweichungen zwischen den berichteten und den neu berechneten Ergebnissen nur bei Fehlern der Autoren bei der Berechnung ihrer Ergebnisse zu erwarten. Reproduktionen eignen sich also dazu, die zweite Quelle an Unsicherheit zu reduzieren.

Eine Berechnung lässt sich auf unterschiedlichen Abstraktionsstufen beschreiben. Da Computer heutzutage die meisten Berechnungen übernehmen, ist die genaueste erhältliche Beschreibung in der Regel Quellcode. Stellen die Autoren einer Studie ihren Quellcode zur Verfügung, kann dieser einfach erneut ausgeführt werden, um die Berechnung zu wiederholen. Diese Art von Reproduktionen bezeichnen \textcite{kohrt_conceptual_2024} in ihrem Conceptual"=Framework"=for"=Computational"=Reproductions als Re"=Execution"=Reproduction. Auch wenn bei einer solchen Reproduktion lediglich der originale Code noch einmal ausgeführt wird, kann sie dennoch abweichende Ergebnisse hervorbringen. Diese Abweichungen lassen sich auf Fehler von Seiten der Autoren beim Berichten der Ergebnisse zurückführen, wenn sie das Maß an Abweichung übersteigen, das durch die bloße Verwendung eines anderen Computers oder anderer Versionen von Software zu erwarten war.

Hat man keinen Zugriff auf den Quellcode, muss man die genaue Implementierung der Berechnung anhand ihrer abstrakteren Beschreibungen rekonstruieren, die sich bspw. im Methodenteil eines Artikels befinden und mitunter einige Details der Berechnung nicht genau spezifizieren. In diesem Fall müssen Abweichungen der Ergebnisse nicht unbedingt auf Fehler von Seiten der Autoren zurückgehen, sondern können auch durch Fehler bei der Rekonstruktion der Berechnung verursacht worden sein. Diese Unsicherheit kann vermieden werden, wenn der Quellcode zu einer Berechnung vorliegt. Dementsprechend betonen \textcite{kohrt_conceptual_2024} auch die zentralen Vorteile von Quellcode für die Reproduzierbarkeit von Berechnungen.

Vor diesem Hintergrund muss positiv hervorgehoben werden, dass \textcite{brotzeller_exploring_2025} den Quellcode aller vier Studien zur Verfügung stellen und damit die Reproduzierbarkeit ihrer Ergebnisse erhöhen. Bei der Reproduktion von Studie 4 soll trotzdem möglichst darauf verzichtet werden, die Implementierung aus dem Quellcode der Autoren wiederzuverwenden. Stattdessen wird versucht, die Berechnung anhand ihrer abstrakteren Beschreibungen zu rekonstruieren und in R neu zu implementieren \autocite{r_core_team_r_2025}. Eine solche Re"=Implementation"=Reproduction hat den Vorteil, dass sie neben Fehlern beim Berichten der Ergebnisse auch Fehler bei der Implementierung der Berechnung aufdecken kann \autocite{kohrt_conceptual_2024}.

Bei einer Re"=Implementation"=Reproduction geht man davon aus, dass es einen richtigen Weg gibt, die Berechnung zu implementieren. Richtige Beschreibungen führen auf diesen Weg, während falsche Beschreibungen von ihm wegführen. Trifft man während der Re"=Implementierung auf inkohärente Beschreibungen, öffnet sich vor einem eine Weggabelung, die entweder zur richtigen oder zur falschen Implementierung der Berechnung führt. Die Autoren einer Studie haben Implementierungsfehler begangen, wenn sie während ihrer Implementierung an mindestens einer Weggabelung der falschen Beschreibung gefolgt sind. Bei der Re"=Implementierung geht es darum, herauszufinden, welche Beschreibungen die richtigen sind. Folgt man erfolgreich den richtigen Beschreibungen, sollte die erneute Durchführung der Berechnung abweichende Ergebnisse produzieren, wenn den Autoren einer Studie Implementierungsfehler unterlaufen sind.

Weil Inkohärenzen von Beschreibungen Weggabelungen während der Implementierung einer Berechnung aufwerfen, sind sie mögliche Quellen für Implementierungsfehler. Kommt eine Re"=Implemantation"=Reproduction zu abweichenden Ergebnissen, kann man sich auf die Suche danach begeben, bei welcher Weggabelung die Autoren einer Studie der falschen Beschreibung gefolgt sind. Das Auffinden von Fehlern ist also eng mit dem Identifizieren von Inkohärenzen verbunden \autocite{kohrt_conceptual_2024}. Je mehr Beschreibungen man bei der Re-Implementierung miteinbezieht, desto mehr Inkohärenzen kann man als potenzielle Fehlerquellen identifizieren. Deshalb werden bei der Reproduktion von Studie 4 neben den Beschreibungen im Artikel von \textcite{brotzeller_exploring_2025} auch die Beschreibungen in der Präregistrierung, im Codebook und den Zusatzmaterialien zu Studie 4 berücksichtigt und auf Inkohärenzen überprüft.

Eine Re"=Implementation"=Reproduction eignet sich dazu, neben Fehlern beim Berichten der Ergebnisse auch Implementierungsfehler aufzudecken. Das bedeutet umgekehrt, dass sie eine höhere verifizierende Funktion als eine Re"=Execution"=Reproduction hat, wenn sie übereinstimmende Ergebnisse produziert. Die Übereinstimmung spricht nämlich nicht nur dafür, dass die Autoren einer Studie ihre Ergebnisse richtig berichtet haben, sondern auch dafür, dass sie diese fehlerfrei berechnet haben.

Weiter oben wurde die Schwierigkeit beschrieben, die genaue Implementierung einer Berechnung anhand ihrer abstrakten Beschreibungen zu rekonstruieren. Muss man während der Re-Implementierung viele eigenständige Entscheidungen treffen, erhöht sich die Wahrscheinlichkeit, dass abweichende Ergebnisse auf eigene Rekonstruktionsfehler statt Implementierungsfehler auf Seiten der Autoren zurückgehen. Umgekehrt kann man argumentieren: Stimmen die Ergebnisse im Rahmen der Abweichungen überein, die aufgrund eigenständiger Entscheidungen bei der Re-Implementierung zu erwarten waren, spricht das umso mehr für die Verlässlichkeit der berichteten Ergebnisse; denn offensichtlich konnten sie auch dann noch reproduziert werden, wenn sich die Neuberechnung leicht von der ursprünglichen Berechnung bei den Autoren unterschied.

Da \textcite{brotzeller_exploring_2025} ihren Quellcode zur Verfügung stellen, erhält man durch einen Blick in den Code auf jede offene Frage während der Re"=Implementierung eine Antwort. Es soll dennoch versucht werden, den Code nur in den Fällen zu Rate zu ziehen, in denen offene Fragen nicht eigenständig beantwortet werden können. Dadurch erhöht sich die Wahrscheinlichkeit, dass die Re"=Implementierung Unterschiede zum Quellcode der Autoren aufweisen wird. Bringt die Reproduktion übereinstimmende Ergebnisse hervor und können im Nachhinein Unterschiede zu der Implementierung der Autoren festgestellt werden, spräche das umso mehr für die Verlässlichkeit der Ergebnisse.

Die Re"=Implementation"=Reproduction von Studie 4 ist in \cref{app:b} vollständig dokumentiert. Sie folgt den Schritten, die das Conceptual"=Framework-for"=Computational"=Reproductions \autocite{kohrt_conceptual_2024} vorgibt. Auf dieser Grundlage werden in den nächsten Abschnitten der Verlauf und die Ergebnisse der Reproduktion zusammengefasst. Dabei wird betont, welche Inkohärenzen und Hindernisse identifiziert wurden.

Die Zusammenfassung der Reproduktion beginnt mit der Darstellung ihres Umfangs. Daran schließt sich die Beschreibung der tatsächlichen Implementierung und Durchführung der Berechnung an. Daraufhin werden die neu berechneten Ergebnisse mit den Ergebnissen verglichen, die \textcite{brotzeller_exploring_2025} berichten, gefolgt von einem Resümee über den Erfolg der Reproduktion. Abschließend werden die Inferenzregeln diskutiert, denen die Autoren bei ihren Schlussfolgerungen aus den Ergebnissen folgen.

Es muss positiv hervorgehoben werden, dass das Bemühen von \textcite{brotzeller_exploring_2025} um Open"=Science die Reproduktion ihrer Ergebnisse maßgeblich erleichtert hat. Ihre Dokumentation der Berechnung ist ausführlich und präzise. Die Daten für Studie 4 sind in anonymisierter Form erhältlich, ebenso wie der gesamte Quellcode und weitere Quellen mit Beschreibungen. 

\subsection{Umfang der Reproduktion und Weiterverarbeitung der Primärdaten}
\label{sec:repro-horizon}

\Textcite{brotzeller_exploring_2025} führen Studie 4 in erster Linie durch, um eine Schlussfolgerung über die Gültigkeit ihrer Theorie zu ziehen. Dazu passen sie das oben umrissene Untersuchungsparadigma so an, dass sie experimentell manipulieren können, wie viele Verbesserungsmöglichkeiten die Versuchspersonen wahrnehmen.

Die Teilnehmer der Studie werden zunächst gebeten, ihre Fähigkeit einzuschätzen, Emotionen anderer Personen anhand ihrer Augenpartie zu erkennen. Anschließend absolvieren sie die Kurzversion des Reading"=the"=Mind"=in"=the"=Eyes"=Tests \autocite{bolte_reading_2005} und erhalten eine Rückmeldung über die gemessene Leistung in diesem Fähigkeitsbereich. Unmittelbar danach, d.h. noch bevor sich die Versuchspersonen selbst-evaluieren und ihr Selbstkonzept ändern, findet die experimentelle Manipulation der wahrgenommenen Verbesserungsmöglichkeiten statt. Der einen Hälfte der Teilnehmer wird mitgeteilt, dass es möglich ist, die in Frage stehende Fähigkeit zu verbessern (z. B. durch Übung); der anderen Hälfte wird gesagt, dass dies selbst bei aktivem Training nahezu unmöglich ist. Erst daraufhin werden die Versuchsteilnehmer um eine zweite Selbsteinschätzung gebeten, um ihre Selbstkonzeptänderungen sichtbar zu machen. Um zu überprüfen, ob die experimentelle Manipulation erfolgreich war, wird zuletzt noch erhoben, wie viele Verbesserungsmöglichkeiten die Personen beider Gruppen tatsächlich wahrnahmen.

\Textcite{brotzeller_exploring_2025} führen nach der Datenerhebung eine Regressionsanalyse durch. Das Regressionsmodell sagt den Betrag der vorgenommenen Selbstkonzeptänderungen vorher und beinhaltet den stetigen Prädiktor \enquote{Größe der Diskrepanz} sowie die beiden dichotomen, effektkodierten Prädiktoren \enquote{Richtung der Diskrepanz} (negativ vs. positiv) und \enquote{wahrgenommene Verbesserungsmöglichkeiten} (kodiert für die beiden experimentellen Gruppen). Damit stellt es eine Erweiterung des Regressionsmodells im mittleren Kasten in \cref{fig:phaenomenons} um eine Variable dar. Außerdem werden neben der Interaktion zwischen der Größe und Richtung der Diskrepanz auch die Interaktionen dieser Prädiktoren mit den wahrgenommenen Verbesserungsmöglichkeiten in das Modell aufgenommen. Die Ergebnisse dieser Regressionsanalyse berichten \textcite{brotzeller_exploring_2025} in Form von Tabelle 2 auf S. 1739.

Über den Wert von zwei der berechneten Regressionskoeffizienten haben \textcite{brotzeller_exploring_2025} spezifische Erwartungen formuliert. Auf Grundlage der Werte, die sich realisiert haben, ziehen sie Schlussfolgerungen über die Gültigkeit ihrer beiden Hypothesen.

Von besonderem Interesse für die Falsifizierung der Theorie ist der Interaktionseffekt zwischen den wahrgenommenen Verbesserungsmöglichkeiten und der Richtung der Diskrepanz. Wäre die Theorie eine zutreffende Beschreibung der Welt, müsste in der Gruppe, die Möglichkeiten zur Verbesserung wahrnahm, ein Negativbias beobachtbar sein, während in der Gruppe, die keine solchen Möglichkeiten wahrnahm, ein Positivbias zu erkennen wäre. Die Voraussetzung dafür, diese spezifischen Zusammenhänge zu messen, wäre die Beobachtung, dass es überhaupt einen Moderationseffekt der Wahrnehmung von Verbesserungsmöglichkeiten auf den Effekt der Richtung der Diskrepanz gibt. Da der Interaktionseffekt zwischen beiden Prädiktoren nicht signifikant war, konnte dieser Moderationseffekt nicht beobachtet werden. Die Autoren schließen daraus, dass die Theorie keine treffende Beschreibung der Welt ist.

Aus dem signifikant positiven Haupteffekt der Größe der Diskrepanz schließen \textcite{brotzeller_exploring_2025}, dass größere Diskrepanzen zwischen Feedback und Selbsteinschätzung mit größeren Änderungen des Selbstkonzepts einhergehen. Dieser Befund repliziert die Ergebnisse aus den Studien 1--3 und der bisherigen Literatur.

Die Berechnung, die im Zuge der Reproduktion wiederholt wird, ist die beschriebene Regressionsanalyse. Sie wird re"=implementiert und erneut durchgeführt. In diesem Zusammenhang werden die Ergebnisse der Regressionsanalyse reproduziert, die \textcite{brotzeller_exploring_2025} in Tabelle 2 auf Seite 1739 berichten.

Eine Re"=Implementation"=Reproduction der Regressionsanalyse würde sich auf einen einzelnen R-Befehl beschränken. Deshalb wird der Umfang der Reproduktion in der vorliegenden Arbeit um die Schritte bei der Weiterverarbeitung der Primärdaten erweitert, die die Autoren als Vorbereitung auf ihre Datenanalyse vornehmen. Das beinhaltet das Anwenden der präregistrierten Ausschlusskriterien auf den bereitgestellten Datensatz sowie die Neuberechnung der Variablen des Regressionsmodells aus den Primärdaten, die dieser Datensatz noch enthält. Beide Verarbeitungsschritte konnten erfolgreich reproduziert werden. Allerdings ergaben sich bei der Neuberechnung der Variablen zwei nennenswerte Schwierigkeiten.

Die Beschreibungen der Berechnung der Richtung der Diskrepanz im Artikel und im Codebook sind inkohärent. Inhaltliche Überlegungen führen zu der Erkenntnis, dass die Beschreibung im Artikel die richtige ist. Im Codebook wird die Richtung der Diskrepanz nicht dem Vorzeichen der Differenz zwischen Feedback und Selbstwahrnehmung entnommen, sondern stattdessen der Differenz zwischen den Selbstwahrnehmungen vor und nach dem Feedback. Folgt man der Beschreibung im Artikel, gelangt man zu denselben Daten wie \textcite{brotzeller_exploring_2025}. Den Autoren ist an dieser Stelle also offensichtlich kein Implementierungsfehler unterlaufen.

Neben dieser Inkohärenz ergab sich noch ein Hindernis, das schwieriger zu überwinden war: Die genaue Berechnungsmethode der Größe der Diskrepanz wurde nicht spezifiziert. Folgt man den gegebenen Beschreibungen, produziert man keine übereinstimmenden Daten. Ein Blick in den originalen Datensatz deckte auf, dass die Werte für die Größe der Diskrepanz auf ganze Zahlen gerundet wurden. Erst nach einigen Fehlversuchen konnte die richtige Rundungsmethode identifiziert und implementiert werden.

\subsection{Implementierung und Durchführung der Berechnung}
\label{sec:repro-implementation}

Die Regressionsanalyse wurde mit dem R-Standardbefehl für lineare Regressionsmodelle re"=implementiert. Dabei ergaben sich keine offenen Fragen, da \textcite{brotzeller_exploring_2025} alle relevanten Parameter des Modells genau genug beschreiben. Das beinhaltet auch die Kodierung der dichotomen Prädiktorvariablen sowie das Verfahren bei der Standardisierung der Größe der Diskrepanz.

Der R-Standardbefehl für Regressionsanalysen hat den Nachteil, dass er die Semipartialkorrelationen nicht mitberechnet, die \textcite{brotzeller_exploring_2025} als Effektstärken für alle geschätzten Regressionskoeffizienten angeben. Aus Ermangelung der notwendigen Fähigkeiten und des Wissens, wie man diese Semipartialkorrelationen berechnet, musste die Methode übernommen werden, die die Autoren in ihrem Code verwenden. Dieser Teil der Berechnung kann also nur dann abweichende Ergebnisse hervorbringen, wenn die vorherige Re-Implementierung der Regressionsanalyse abweichende Ergebnisse produzierte.

\subsection{Vergleich der Ergebnisse}
\label{sec:repro-comparison}

Die re"=implementierte Regressionsanalyse kam zu Ergebnissen, die mit den berichteten Ergebnissen konsistent sind. Rundet man die berechneten Werte auf dieselbe Anzahl an Nachkommastellen wie \textcite{brotzeller_exploring_2025}, sind die Werte sogar numerisch identisch. Das bedeutet, dass die Ergebnisse erfolgreich reproduziert wurden und den Autoren keine Fehler bei der Implementierung der Regressionsanalyse oder beim Berichten der Ergebnisse unterlaufen sind. Das Vertrauen in die Verlässlichkeit der Ergebnisse wird dadurch gestärkt. Das erhöht gleichzeitig das Vertrauen in die Gültigkeit der Schlussfolgerungen, die \Citeauthor{brotzeller_exploring_2025} aus den Ergebnissen ziehen.

In der Einleitung des Kapitels wurde dafür argumentiert, dass das Vertrauen in die Verlässlichkeit der berichteten Ergebnisse noch weiter untermauert wird, wenn sie trotz feststellbarer Unterschiede zwischen der originalen Implementierung und der Re"=Implementierung mit den neu berechneten Ergebnissen übereinstimmen. Bei einem Post"=Hoc"=Vergleich zwischen dem Quellcode von \textcite{brotzeller_exploring_2025} und dem re"=implementierten Code in \cref{app:b} konnten jedoch keine wesentlichen Unterschiede festgestellt werden. Für die Regressionsanalyse verwenden die Autoren ebenfalls den R"=Standardbefehl und die Methode zur Berechnung der Semipartialkorrelationen wurde bei der Reproduktion einfach wiederverwendet. Es überrascht also nicht, dass die Ergebnisse übereinstimmen.

Was hingegen das Vertrauen in die Verlässlichkeit der Ergebnisse stärkt, ist die Tatsache, dass die Berechnung der Variablen des Regressionsmodells anhand der Beschreibungen im Codebook zu denselben Werten führt, welche die Autoren in dem bereitgestellten Datensatz angeben. Die Autoren haben sich also offensichtlich ebenfalls an die Beschreibungen im Codebook gehalten.

\subsection{Diskussion der Inferenzregeln}
\label{sec:repro-evaluation}

\Textcite{brotzeller_exploring_2025} ziehen aus dem signifikant positiven Effekt der Größe der Diskrepanz die Schlussfolgerung, dass größere Diskrepanzen mit größeren Änderungen des Selbstkonzepts einhergehen. Die Regeln, nach denen sie diese Inferenz ziehen, sind die Regeln dafür, wie Regressionskoeffizienten korrekt interpretiert werden. Da die Interpretation von \citeauthor{brotzeller_exploring_2025} diesen Regeln folgt, ist ihre Schlussfolgerung einwandfrei.

Es besteht allerdings die Gefahr, dass Rezipienten des Artikels die Interpretation der Autoren auf die folgende Weise missverstehen: Größere Diskrepanzen gehen mit größeren Änderungen des Selbstkonzepts in Richtung des diskrepanten Feedbacks einher. Diese Interpretation wäre auf der Grundlage des verwendeten Regressionsmodells nicht gültig.
In dem Regressionsmodell, das \textcite{brotzeller_exploring_2025} verwenden, wird der Betrag der Differenz zwischen den Selbstwahrnehmungen vor und nach dem Feedback vorhergesagt. Die Regressionsanalyse berücksichtigt also nicht die Richtung der vorgenommenen Selbstkonzeptänderungen. Man würde erwarten, dass Versuchspersonen ihre Selbstwahrnehmung ohnehin immer nur in Richtung des Feedbacks anpassen, das sie erhalten. Eine Analyse des Datensatzes zeigt jedoch, dass 17.4\,\% (79 von 453) der Versuchspersonen in der finalen Stichprobe von Studie 4 ihr Selbstkonzept in die gegenteilige Richtung änderten, als das Feedback anzeigte. Das Modell wirft diese gegensätzlich gerichteten Änderungen des Selbstkonzepts in einen Topf mit allen gleich gerichteten Selbstkonzeptänderungen. Um Fehlinterpretationen zu vermeiden, sollte die verwendete Interpretation deutlich machen, dass größere Diskrepanzen mit größeren \emph{absoluten} Selbstkonzeptänderungen einhergehen.

\textcite{brotzeller_exploring_2025} schließen aus dem nicht signifikanten Interaktionseffekt zwischen den wahrgenommenen Verbesserungsmöglichkeiten und der Richtung der Diskrepanz, dass die Theorie die Realität nicht adäquat abbildet. Die Inferenzregel, der die Autoren dabei folgen und die sie als \enquote{indirekte Inferenz} bezeichnen, kann man folgendermaßen ausformulieren: Wenn das Phänomen nicht beobachtbar ist, das eigentlich auftreten müsste, wenn die Theorie die Realität tatsächlich abbilden würde, kann die Theorie keine treffende Beschreibung der Realität sein. Diese Inferenzregel weist Ähnlichkeiten zu der Regel auf, nach der die Erklärungskraft von Theorien im Productive"=Explanation"=Framework bemessen wird: Wenn man in einer Simulation ein Abbild der Realität nach dem formalen Modell der Theorie erstellt, müsste in den simulierten Daten das statistische Muster auftreten, das das Phänomen repräsentiert \autocite{van_dongen_productive_2025}. Auch hier erfolgt der Nachweis der Gültigkeit der Theorie über den Nachweis der Existenz des Phänomens, das aus ihr folgen sollte. Allerdings gibt es einen entscheidenden Unterschied.

Das Productive"=Explanation"=Framework geht i.d.R. von dem Szenario aus, in dem bereits ein stabiles Phänomen beobachtet wurde. Dieses Phänomen ist der Fixpunkt, an dem in einer Datensimulation bemessen wird, wie adäquat die Theorie die Realität abbildet. Kann die Datensimulation das Phänomen nicht produzieren, muss das auf Unzulänglichkeiten der theoretischen Beschreibung der Realität zurückgehen, weil die Realität nun einmal so aussieht, wie sie aussieht (das Phänomen ist \enquote{fix}).

Der Ausgangspunkt der Untersuchungen von \textcite{brotzeller_exploring_2025} ist nun aber, dass noch keine stabilen Zusammenhänge zwischen einem Moderator und der Richtung der Asymmetrie in den Selbstkonzeptänderungen von Versuchspersonen beobachtet werden konnten. Die Autoren versuchen, ein solches Phänomen zu identifizieren und gehen dabei theoriegeleitet vor: Wenn Self"=Improvement und Self"=Enhancement für den Positv"= und den Negativbias verantwortlich sind, müsste das Phänomen beobachtbar sein, das in \cref{fig:placeholder-3} definiert wurde. Die Theorie geht also dem Phänomen voraus.

Das bedeutet, dass bei der oben skizzierten Datensimulation lediglich angenommen wurde, dass das Phänomen tatsächlich existiert. Folglich kann sie nur in Verbindung mit der tatsächlichen Messung, ob das Phänomen auch wirklich existiert oder nicht, zu einer vollständigen Aussage über die Erklärungskraft der Theorie kommen. Könnte das formale Modell der Theorie das statistische Muster des Phänomens produzieren, wäre die Theorie dennoch ungültig, wenn das Phänomen nicht beobachtbar wäre.

Umgekehrt gilt jedoch: Nur weil das Phänomen, das aus der Theorie abgeleitet wurde, gemessen werden konnte, bedeutet das nicht automatisch, dass die Theorie das Phänomen schlüssig erklärt. Um das nachzuweisen, bedarf es einer Datensimulation nach der Definition des Productive-Explanation-Framework. Beide Ansätze zur Überprüfung der Theorie belegen die Gültigkeit der Theorie einzeln genommen also nicht hinreichend.

Nimmt man die Messung des Phänomens von \textcite{brotzeller_exploring_2025} in Studie 4 und die Datensimulation zusammen -– zu welcher Aussage über die Gültigkeit der Theorie würde man gelangen? Da kein Interaktionseffekt zwischen den wahrgenommenen Verbesserungsmöglichkeiten und der Richtung der Diskrepanz gemessen wurde, spricht das gegen die Existenz des Phänomens. Bedenkt man, dass die Theorie das statistische Muster des Phänomens in einer Datensimulation nicht produzieren könnte, kommt man zu dem Schluss, dass die Theorie eine geringe Erklärungskraft aufweist. Beides zusammen spricht gegen die Gültigkeit der Theorie.

Diese Schlussfolgerung wird hier allerdings noch nicht gezogen, denn sie wäre nur dann gültig, wenn Studie 4 letztgültig belegen könnte, dass das Phänomen nicht existiert. Das Phänomen lässt sich nur dann identifizieren, wenn die Theorie genau genug spezifiziert, unter welchen Bedingungen ein Negativ"= und ein Positivbias auftreten sollten. Für die Theorie der Autoren bedeutet das, dass sie genau genug spezifizieren muss, unter welchen Bedingungen Self"=Improvement und unter welchen Bedingungen Self"=Enhancement auftreten wird. \Textcite{brotzeller_exploring_2025} geben präzise Definitionen dieser Bedingungen (\cref{item:a5} und \cref{item:a6}). Jedoch wurde festgestellt, dass es in der Theorie der Autoren um die subjektiv wahrgenommenen Verbesserungsmöglichkeiten geht, was die Vorhersage erschwert, unter welchen Versuchsbedingungen ein Positivbias oder ein Negativbias auftreten wird. In Einklang mit dieser Argumentation melden \textcite{brotzeller_exploring_2025} berechtigte Zweifel daran an, dass die Manipulation der subjektiven Wahrnehmung geringer Verbesserungsmöglichkeiten in Studie 4 erfolgreich war, was wiederum daran zweifeln lässt, dass Studie 4 die Existenz des Phänomens widerlegen konnte.
